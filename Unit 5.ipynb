{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a24468-fdc9-4788-a68d-6873c022b85c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-trained models for automatic speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac593ce-0ddc-41a3-b57b-7a8bf47f57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2fcec-64f4-48d6-a0bb-8e6b5ab0af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8c5d7-da5b-43ce-8a17-bea904fdc037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install soundfile\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29691f33-e0f5-49e4-8d44-9b6017662da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcae10-8104-439c-a62b-90b5f3bd3847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f71b4c-fb87-47eb-b5ef-abcd20f52879",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Probing CTC Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50e48b-eac4-4ff9-8d87-0b288c602643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af40480-07d3-435c-b5aa-9ae31b5be05a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "sample = dataset[2]\n",
    "print(sample[\"text\"])\n",
    "IPython.display.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c595f7-f330-4db6-86af-022b3458827d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-100h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a26f7f-34d8-401f-942d-3f382417c03a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf4de1-0459-408c-b49a-768ca5bcf5b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Graduation to Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fdea5e-9d8f-4bf6-ac8a-a94cb757faf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72662865-c737-45aa-a92b-a93c9af75bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e7619-d842-4177-89ca-0146a910c3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"], max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5fcc9-3a83-4049-b749-5444bdaf12d3",
   "metadata": {},
   "source": [
    "#### Multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87510a-203f-4323-8296-3d4974bf652a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"facebook/multilingual_librispeech\", \"dutch\", split=\"validation\", streaming=True)\n",
    "sample = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad820065-35ab-4a6b-a7eb-d1963c741e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sample[\"text\"])\n",
    "IPython.display.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558382a-6d3e-4068-8f9e-59e3239ec584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\":\"transcribe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb430b-cd43-4eee-8069-c92fe5568340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\":\"translate\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea787e-aeeb-4251-be6c-daa709369352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"facebook/multilingual_librispeech\", \"spanish\", split=\"validation\", streaming=True)\n",
    "sample = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca9664-c583-4f47-9cef-f00c2683290e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sample[\"text\"])\n",
    "IPython.display.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efa031-2bec-4464-88a3-77069ee5ac39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\":\"transcribe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100704af-398b-4572-a105-7b87ffe22407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\":\"translate\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529302de-f874-4868-bd55-b852d81121bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Long-Form Transcription and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8407c18-b830-4d16-8e98-bf7a679c5579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target_length_in_m = 5\n",
    "# convert from minutes to seconds(* 60) to num samples(* sampling_rate)\n",
    "sampling_rate = pipe.feature_extractor.sampling_rate\n",
    "target_length_in_samples = target_length_in_m * 60 * sampling_rate\n",
    "long_audio = []\n",
    "for sample in dataset:\n",
    "    long_audio.extend(sample[\"audio\"][\"array\"])\n",
    "    if len(long_audio) > target_length_in_samples:\n",
    "        break\n",
    "        \n",
    "long_audio = np.asarray(long_audio)\n",
    "\n",
    "# how did we do\n",
    "\n",
    "seconds = len(long_audio) / 16_000\n",
    "minutes, seconds = divmod(seconds, 60)\n",
    "\n",
    "print(f\"Length of audio sample is {minutes} minutes {seconds:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a5e23-b06e-475a-81fb-4f8b6506b711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe (\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30, # 30 second chunks\n",
    "    batch_size=8 # batch of 8 chunks at a time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d583442-8d52-4a24-954d-7dd9d96c8ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe (\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30, # 30 second chunks\n",
    "    batch_size=8, # batch of 8 chunks at a time\n",
    "    return_timestamps=True #return timestamps for annotating video\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2614998-ac2a-414a-afde-33d38347866d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluation and metrics for speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7694e-bc70-48d7-97e2-0ebbe90d9570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference = \"the cat sat on the mat\"\n",
    "prediction = \"the cat sit on the\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455654ee-747f-4c07-8705-0d55f0a17720",
   "metadata": {},
   "source": [
    "### Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4884a2-ce3d-43a6-af8f-b0bae12513fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade evaluate jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88c921-7c03-4b06-87f8-95c164f89f70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "wer_metric = load(\"wer\")\n",
    "wer = wer_metric.compute(references=[reference], predictions=[prediction])\n",
    "wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5a42c-8d95-4be1-abf6-e12cd2281a03",
   "metadata": {},
   "source": [
    "### Word accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1195f0-c179-48b1-8986-acc09ab295a6",
   "metadata": {},
   "source": [
    "W Acc = 1 - WER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aade4a9f-da64-4cf3-ba87-2743c2834db2",
   "metadata": {},
   "source": [
    "### Character error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a1cd5-9f39-4562-a162-62c72cc006c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cer_metric = load(\"cer\")\n",
    "cer = cer_metric.compute(references=[reference], predictions=[prediction])\n",
    "cer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96fa456-04bd-4b0a-9943-cf06ab53065a",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3ba4a-9d4f-4914-a6a5-fc43d3dedf67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "normalizer = BasicTextNormalizer()\n",
    "prediction = \" He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.\"\n",
    "normalized_prediction = normalizer(prediction)\n",
    "normalized_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63b367-0688-48b8-85eb-882fc247f5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference = \"HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\"\n",
    "normalized_reference = normalizer(reference)\n",
    "\n",
    "wer = wer_metric.compute(\n",
    "    references=[normalized_reference], predictions=[normalized_prediction]\n",
    ")\n",
    "wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be221e-fe61-45b1-b3ab-a88498934388",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71d2fa-6499-4b8c-8039-54b2afd5117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to login\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837d27a-d360-4fc3-bbe4-75d48bf378f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "    torch_dtype=torch.float32\n",
    "    \n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa1fb6-98cf-47f3-b63f-5b907801d6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "common_voice_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faae6e6-5120-487e-a238-65774387b73c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "all_predictions =[]\n",
    "\n",
    "# rucommon_voice_testreamed inference\n",
    "for prediction in tqdm(\n",
    "    pipe(\n",
    "        KeyDataset(common_voice_test, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\"},\n",
    "        batch_size=32,\n",
    "    ),\n",
    "    total=len(common_voice_test),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a4bbc-e6ec-44d3-b0ac-da5a117e12b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "wer_metric = load(\"wer\")\n",
    "wer_ortho = 100* wer_metric.compute(\n",
    "    references=common_voice_test[\"sentence\"], predictions=all_predictions\n",
    ")\n",
    "wer_ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bb4d7-d2cb-4107-bb75-b452951df532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf195a-abbf-48ae-906f-ff6ad7f62547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute normalised WER\n",
    "all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
    "all_references_norm = [normalizer(label) for label in common_voice_test[\"sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5e319-eb1f-447c-901f-eade6dbacac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtering step to only evaluate the samples that correspond to non-zero-references\n",
    "all_predictions_norm = [\n",
    "    all_predictions_norm[i] for i in range(len(all_predictions_norm)) if len(all_references_norm[i]) > 0\n",
    "]\n",
    "all_references_norm = [\n",
    "    all_references_norm[i] for i in range(len(all_references_norm)) if len(all_references_norm[i]) > 0\n",
    "]\n",
    "wer = 100 * wer_metric.compute(\n",
    "    references=all_references_norm, predictions=all_predictions_norm\n",
    ")\n",
    "\n",
    "wer\n",
    "                                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050c63e-e120-48e3-ba38-d8912d7d34f4",
   "metadata": {},
   "source": [
    "## Fine-tune ASR with Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1cb43-4fdd-41da-9729-ffd24063f1c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4dfa2da-00d3-4301-837a-b057670fc3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62cd58098f640a28b58584172d70b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93abfd-5126-4302-b933-45be8cf01b95",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d416827-5724-474c-b4cd-136c50cc2ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset common_voice_13_0 (/root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055)\n",
      "Reusing dataset common_voice_13_0 (/root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 4904\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 2212\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"train+validation\", use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\", use_auth_token=True)\n",
    "\n",
    "common_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63f68cd-e5a8-48ea-ba7d-75b9cd4d26cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 4904\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 2212\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns(['client_id', 'path', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'])\n",
    "common_voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4b199-c68e-45c0-ba9e-21aab966480c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Extractor, Tokenizer and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb1fcb7-ad03-420d-af68-ff68b43a5524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'en',\n",
       " 'chinese': 'zh',\n",
       " 'german': 'de',\n",
       " 'spanish': 'es',\n",
       " 'russian': 'ru',\n",
       " 'korean': 'ko',\n",
       " 'french': 'fr',\n",
       " 'japanese': 'ja',\n",
       " 'portuguese': 'pt',\n",
       " 'turkish': 'tr',\n",
       " 'polish': 'pl',\n",
       " 'catalan': 'ca',\n",
       " 'dutch': 'nl',\n",
       " 'arabic': 'ar',\n",
       " 'swedish': 'sv',\n",
       " 'italian': 'it',\n",
       " 'indonesian': 'id',\n",
       " 'hindi': 'hi',\n",
       " 'finnish': 'fi',\n",
       " 'vietnamese': 'vi',\n",
       " 'hebrew': 'he',\n",
       " 'ukrainian': 'uk',\n",
       " 'greek': 'el',\n",
       " 'malay': 'ms',\n",
       " 'czech': 'cs',\n",
       " 'romanian': 'ro',\n",
       " 'danish': 'da',\n",
       " 'hungarian': 'hu',\n",
       " 'tamil': 'ta',\n",
       " 'norwegian': 'no',\n",
       " 'thai': 'th',\n",
       " 'urdu': 'ur',\n",
       " 'croatian': 'hr',\n",
       " 'bulgarian': 'bg',\n",
       " 'lithuanian': 'lt',\n",
       " 'latin': 'la',\n",
       " 'maori': 'mi',\n",
       " 'malayalam': 'ml',\n",
       " 'welsh': 'cy',\n",
       " 'slovak': 'sk',\n",
       " 'telugu': 'te',\n",
       " 'persian': 'fa',\n",
       " 'latvian': 'lv',\n",
       " 'bengali': 'bn',\n",
       " 'serbian': 'sr',\n",
       " 'azerbaijani': 'az',\n",
       " 'slovenian': 'sl',\n",
       " 'kannada': 'kn',\n",
       " 'estonian': 'et',\n",
       " 'macedonian': 'mk',\n",
       " 'breton': 'br',\n",
       " 'basque': 'eu',\n",
       " 'icelandic': 'is',\n",
       " 'armenian': 'hy',\n",
       " 'nepali': 'ne',\n",
       " 'mongolian': 'mn',\n",
       " 'bosnian': 'bs',\n",
       " 'kazakh': 'kk',\n",
       " 'albanian': 'sq',\n",
       " 'swahili': 'sw',\n",
       " 'galician': 'gl',\n",
       " 'marathi': 'mr',\n",
       " 'punjabi': 'pa',\n",
       " 'sinhala': 'si',\n",
       " 'khmer': 'km',\n",
       " 'shona': 'sn',\n",
       " 'yoruba': 'yo',\n",
       " 'somali': 'so',\n",
       " 'afrikaans': 'af',\n",
       " 'occitan': 'oc',\n",
       " 'georgian': 'ka',\n",
       " 'belarusian': 'be',\n",
       " 'tajik': 'tg',\n",
       " 'sindhi': 'sd',\n",
       " 'gujarati': 'gu',\n",
       " 'amharic': 'am',\n",
       " 'yiddish': 'yi',\n",
       " 'lao': 'lo',\n",
       " 'uzbek': 'uz',\n",
       " 'faroese': 'fo',\n",
       " 'haitian creole': 'ht',\n",
       " 'pashto': 'ps',\n",
       " 'turkmen': 'tk',\n",
       " 'nynorsk': 'nn',\n",
       " 'maltese': 'mt',\n",
       " 'sanskrit': 'sa',\n",
       " 'luxembourgish': 'lb',\n",
       " 'myanmar': 'my',\n",
       " 'tibetan': 'bo',\n",
       " 'tagalog': 'tl',\n",
       " 'malagasy': 'mg',\n",
       " 'assamese': 'as',\n",
       " 'tatar': 'tt',\n",
       " 'hawaiian': 'haw',\n",
       " 'lingala': 'ln',\n",
       " 'hausa': 'ha',\n",
       " 'bashkir': 'ba',\n",
       " 'javanese': 'jw',\n",
       " 'sundanese': 'su',\n",
       " 'burmese': 'my',\n",
       " 'valencian': 'ca',\n",
       " 'flemish': 'nl',\n",
       " 'haitian': 'ht',\n",
       " 'letzeburgesch': 'lb',\n",
       " 'pushto': 'ps',\n",
       " 'panjabi': 'pa',\n",
       " 'moldavian': 'ro',\n",
       " 'moldovan': 'ro',\n",
       " 'sinhalese': 'si',\n",
       " 'castilian': 'es'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
    "TO_LANGUAGE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ac1726-ab77-4114-b27b-f31931ff6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinhalese is closest\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"sinhalese\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2045f4-ba78-4358-a55c-a08476a2c704",
   "metadata": {},
   "source": [
    "### Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ed72ab-5634-475a-ba41-ea8095f51be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\n",
       " 'sentence': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "549642d9-f1ee-4cc2-a002-0f85230a4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "sampling_rate=processor.feature_extractor.sampling_rate\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ba8fa95-1cb1-4a73-b911-728ce3b0d768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-b89d6dda11a3da8f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-14625530da535f3a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-96add9a17e4edbaf.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-3a45b308363e6de8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-7d61751464c07eab.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-0b1743d1de7a36ec.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-659a88d23de47262.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-8df910fd903683ea.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-006b39c38d4f658f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-ff7bb14c077cd92c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-4f21dc4b10a3f6c9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-7f9bb15c3d105c61.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-0c4ab2b18703552d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-bbfd01eaae8e9b08.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-da31611b4dbeaed2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-7841185c17a5750b.arrow\n"
     ]
    }
   ],
   "source": [
    "# load and resample\n",
    "# compute log mel spectrogram\n",
    "# encode transcriptions to label ids with tokenizer\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "    example = processor(audio=audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], text=example[\"sentence\"])\n",
    "    # comppute input length of audio sample inseconds\n",
    "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "    \n",
    "    return example\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c98ac28-b19f-4314-aff8-c0e68e991eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-eb4257516de3fbd5.arrow\n"
     ]
    }
   ],
   "source": [
    "max_input_length = 30.0\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length\n",
    "\n",
    "# filter\n",
    "\n",
    "common_voice[\"train\"] = common_voice[\"train\"].filter(is_audio_in_length_range, input_columns=[\"input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37bf391c-af42-4242-8cc1-31528a57fb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_features', 'labels', 'input_length'],\n",
       "    num_rows: 4904\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da1b40-f43b-4b9f-831a-e4da43927d8a",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c754b41-8482-441e-b24d-4dea4fc014ed",
   "metadata": {},
   "source": [
    "- Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n",
    "- Evaluation metrics: during evaluation, we want to evaluate the model using the word error rate (WER) metric. We need to define a compute_metrics function that handles this computation.\n",
    "- Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
    "- Define the training arguments: these will be used by the ðŸ¤— Trainer in constructing the training schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9191c-f41f-440a-a0b1-d86affb3921a",
   "metadata": {},
   "source": [
    "#### Define a Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19fa7bfd-9bcc-4c96-853a-2e8a0faff8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    \n",
    "    def __call__(self, features:List[Dict[str,Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\":feature[\"input_features\"][0]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        # get tokenized label sequences\n",
    "        label_features =[{\"input_ids\":feature[\"labels\"]} for feature in features]\n",
    "        # pad labels\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        # replace padding with -100\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        # remove start of transcript token appended by tokenizer as it's appended later anyway\n",
    "        if(labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d297ed6d-8293-434a-9234-068db97cd085",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac86e4e-3d71-4655-958c-b13c793f5096",
   "metadata": {},
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a0bccd-ac40-425e-8f2e-07b77908987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86118eb-1f39-4054-a1e1-67f3fe6302f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a7bf000-6b77-49a1-8fdb-b7dfee3dc1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10f31de0-3251-4c7d-a655-dee55f2c5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbb22a3f-9cf5-49ad-bffd-adcd7084d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    \n",
    "    # replace -100 with pad token\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    # do not group tokens when computing metrics\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # compute ortographic wer\n",
    "    wer_ortho = 100* metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    # compute normalized wer\n",
    "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
    "    label_str_norm = [normalizer(label) for label in label_str]\n",
    "    \n",
    "    # only evaluate non-zero references\n",
    "    pred_str_norm = [pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0]\n",
    "    label_str_norm = [label_str_norm[i] for i in range(len(label_str_norm)) if len(label_str_norm[i]) > 0]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
    "\n",
    "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e977aa9-9735-4e61-b261-e9fde0a57cdd",
   "metadata": {},
   "source": [
    "#### Load a pre-trained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e03d706-d8ea-42b4-baa6-dc1eccd75f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb7161e6-1f22-4856-a2f4-feb7feb7f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# disable cache during training as it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(model.generate, language=\"sinhalese\", task=\"transcribe\", use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044604bc-2c59-41bc-9c09-ab0f9e56160e",
   "metadata": {},
   "source": [
    "### Define a training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0163b8b0-d8c3-4e71-bbad-27eaf60ac632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "398fbc52-cec1-4e69-97e6-c7087716a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-dv\", # name on HF Hub\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1, # increase by 2 for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=50,\n",
    "    max_steps=4000, # decrease to 500 if you don't have your own GPU or a Colab paid plan or equivalent\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"wandb\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "177baa72-8ec5-4ef8-8d6c-025eb2bf9251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 3 not upgraded.\n",
      "Need to get 3316 kB of archives.\n",
      "After this operation, 11.1 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 git-lfs amd64 2.9.2-1 [3316 kB]\n",
      "Fetched 3316 kB in 0s (15.9 MB/s)\n",
      "Selecting previously unselected package git-lfs.\n",
      "(Reading database ... 69943 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_2.9.2-1_amd64.deb ...\n",
      "Unpacking git-lfs (2.9.2-1) ...\n",
      "Setting up git-lfs (2.9.2-1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25a81c7a-748f-4a00-ab19-26aa4bfc8770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated git hooks.\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52aac65a-65ec-4860-bf56-0d369242be4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/ptah23/whisper-small-dv into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c790aa8d-a662-4ee1-b093-9826ffc78183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06de2ea-e250-4d32-9da4-1d8315e11ba1",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074dd210-15d5-4972-9389-16c28dbfce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mptah23\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20230707_214400-3lwoen7c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ptah23/huggingface/runs/3lwoen7c\" target=\"_blank\">dauntless-sea-2</a></strong> to <a href=\"https://wandb.ai/ptah23/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  11/4000 01:42 < 12:37:52, 0.09 it/s, Epoch 0.03/14]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d904eb2e-4f74-4c9b-92e1-35c7fb8384f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
