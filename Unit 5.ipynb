{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a24468-fdc9-4788-a68d-6873c022b85c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-trained models for automatic speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8c5d7-da5b-43ce-8a17-bea904fdc037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install soundfile\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29691f33-e0f5-49e4-8d44-9b6017662da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcae10-8104-439c-a62b-90b5f3bd3847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f71b4c-fb87-47eb-b5ef-abcd20f52879",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Probing CTC Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50e48b-eac4-4ff9-8d87-0b288c602643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af40480-07d3-435c-b5aa-9ae31b5be05a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "sample = dataset[2]\n",
    "print(sample[\"text\"])\n",
    "IPython.display.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c595f7-f330-4db6-86af-022b3458827d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-100h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a26f7f-34d8-401f-942d-3f382417c03a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf4de1-0459-408c-b49a-768ca5bcf5b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Graduation to Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fdea5e-9d8f-4bf6-ac8a-a94cb757faf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72662865-c737-45aa-a92b-a93c9af75bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e7619-d842-4177-89ca-0146a910c3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"], max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5fcc9-3a83-4049-b749-5444bdaf12d3",
   "metadata": {},
   "source": [
    "#### Multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87510a-203f-4323-8296-3d4974bf652a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"facebook/multilingual_librispeech\", \"dutch\", split=\"validation\", streaming=True)\n",
    "sample = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad820065-35ab-4a6b-a7eb-d1963c741e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sample[\"text\"])\n",
    "IPython.display.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558382a-6d3e-4068-8f9e-59e3239ec584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\":\"transcribe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb430b-cd43-4eee-8069-c92fe5568340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\":\"translate\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea787e-aeeb-4251-be6c-daa709369352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"facebook/multilingual_librispeech\", \"spanish\", split=\"validation\", streaming=True)\n",
    "sample = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca9664-c583-4f47-9cef-f00c2683290e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sample[\"text\"])\n",
    "IPython.display.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efa031-2bec-4464-88a3-77069ee5ac39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\":\"transcribe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100704af-398b-4572-a105-7b87ffe22407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\":\"translate\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529302de-f874-4868-bd55-b852d81121bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Long-Form Transcription and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8407c18-b830-4d16-8e98-bf7a679c5579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target_length_in_m = 5\n",
    "# convert from minutes to seconds(* 60) to num samples(* sampling_rate)\n",
    "sampling_rate = pipe.feature_extractor.sampling_rate\n",
    "target_length_in_samples = target_length_in_m * 60 * sampling_rate\n",
    "long_audio = []\n",
    "for sample in dataset:\n",
    "    long_audio.extend(sample[\"audio\"][\"array\"])\n",
    "    if len(long_audio) > target_length_in_samples:\n",
    "        break\n",
    "        \n",
    "long_audio = np.asarray(long_audio)\n",
    "\n",
    "# how did we do\n",
    "\n",
    "seconds = len(long_audio) / 16_000\n",
    "minutes, seconds = divmod(seconds, 60)\n",
    "\n",
    "print(f\"Length of audio sample is {minutes} minutes {seconds:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a5e23-b06e-475a-81fb-4f8b6506b711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe (\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30, # 30 second chunks\n",
    "    batch_size=8 # batch of 8 chunks at a time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d583442-8d52-4a24-954d-7dd9d96c8ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe (\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30, # 30 second chunks\n",
    "    batch_size=8, # batch of 8 chunks at a time\n",
    "    return_timestamps=True #return timestamps for annotating video\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2614998-ac2a-414a-afde-33d38347866d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluation and metrics for speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7694e-bc70-48d7-97e2-0ebbe90d9570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference = \"the cat sat on the mat\"\n",
    "prediction = \"the cat sit on the\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455654ee-747f-4c07-8705-0d55f0a17720",
   "metadata": {},
   "source": [
    "### Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4884a2-ce3d-43a6-af8f-b0bae12513fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade evaluate jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88c921-7c03-4b06-87f8-95c164f89f70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "wer_metric = load(\"wer\")\n",
    "wer = wer_metric.compute(references=[reference], predictions=[prediction])\n",
    "wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5a42c-8d95-4be1-abf6-e12cd2281a03",
   "metadata": {},
   "source": [
    "### Word accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1195f0-c179-48b1-8986-acc09ab295a6",
   "metadata": {},
   "source": [
    "W Acc = 1 - WER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aade4a9f-da64-4cf3-ba87-2743c2834db2",
   "metadata": {},
   "source": [
    "### Character error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a1cd5-9f39-4562-a162-62c72cc006c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cer_metric = load(\"cer\")\n",
    "cer = cer_metric.compute(references=[reference], predictions=[prediction])\n",
    "cer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96fa456-04bd-4b0a-9943-cf06ab53065a",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3ba4a-9d4f-4914-a6a5-fc43d3dedf67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "normalizer = BasicTextNormalizer()\n",
    "prediction = \" He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.\"\n",
    "normalized_prediction = normalizer(prediction)\n",
    "normalized_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63b367-0688-48b8-85eb-882fc247f5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference = \"HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\"\n",
    "normalized_reference = normalizer(reference)\n",
    "\n",
    "wer = wer_metric.compute(\n",
    "    references=[normalized_reference], predictions=[normalized_prediction]\n",
    ")\n",
    "wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be221e-fe61-45b1-b3ab-a88498934388",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71d2fa-6499-4b8c-8039-54b2afd5117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to login\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837d27a-d360-4fc3-bbe4-75d48bf378f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "    torch_dtype=torch.float32\n",
    "    \n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa1fb6-98cf-47f3-b63f-5b907801d6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "common_voice_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faae6e6-5120-487e-a238-65774387b73c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "all_predictions =[]\n",
    "\n",
    "# rucommon_voice_testreamed inference\n",
    "for prediction in tqdm(\n",
    "    pipe(\n",
    "        KeyDataset(common_voice_test, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\"},\n",
    "        batch_size=32,\n",
    "    ),\n",
    "    total=len(common_voice_test),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a4bbc-e6ec-44d3-b0ac-da5a117e12b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "wer_metric = load(\"wer\")\n",
    "wer_ortho = 100* wer_metric.compute(\n",
    "    references=common_voice_test[\"sentence\"], predictions=all_predictions\n",
    ")\n",
    "wer_ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bb4d7-d2cb-4107-bb75-b452951df532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf195a-abbf-48ae-906f-ff6ad7f62547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute normalised WER\n",
    "all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
    "all_references_norm = [normalizer(label) for label in common_voice_test[\"sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5e319-eb1f-447c-901f-eade6dbacac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtering step to only evaluate the samples that correspond to non-zero-references\n",
    "all_predictions_norm = [\n",
    "    all_predictions_norm[i] for i in range(len(all_predictions_norm)) if len(all_references_norm[i]) > 0\n",
    "]\n",
    "all_references_norm = [\n",
    "    all_references_norm[i] for i in range(len(all_references_norm)) if len(all_references_norm[i]) > 0\n",
    "]\n",
    "wer = 100 * wer_metric.compute(\n",
    "    references=all_references_norm, predictions=all_predictions_norm\n",
    ")\n",
    "\n",
    "wer\n",
    "                                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050c63e-e120-48e3-ba38-d8912d7d34f4",
   "metadata": {},
   "source": [
    "## Fine-tune ASR with Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1cb43-4fdd-41da-9729-ffd24063f1c6",
   "metadata": {},
   "source": [
    "### Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dfa2da-00d3-4301-837a-b057670fc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93abfd-5126-4302-b933-45be8cf01b95",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d416827-5724-474c-b4cd-136c50cc2ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_13_0 (/home/studio-lab-user/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055)\n",
      "Found cached dataset common_voice_13_0 (/home/studio-lab-user/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 4904\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 2212\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"train+validation\")\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\")\n",
    "\n",
    "common_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63f68cd-e5a8-48ea-ba7d-75b9cd4d26cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 4904\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 2212\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice = common_voice.select_columns([\"audio\", \"sentence\"])\n",
    "common_voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4b199-c68e-45c0-ba9e-21aab966480c",
   "metadata": {},
   "source": [
    "### Feature Extractor, Tokenizer and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1fcb7-ad03-420d-af68-ff68b43a5524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
