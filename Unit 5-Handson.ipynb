{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a24468-fdc9-4788-a68d-6873c022b85c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hands-on exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac593ce-0ddc-41a3-b57b-7a8bf47f57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2fcec-64f4-48d6-a0bb-8e6b5ab0af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8c5d7-da5b-43ce-8a17-bea904fdc037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install soundfile\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29691f33-e0f5-49e4-8d44-9b6017662da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcae10-8104-439c-a62b-90b5f3bd3847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c12d2c-0122-424e-a6c8-d0ffe99f3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b7baf-0a0c-4f42-af39-22850d0a9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75040f31-6bdf-43df-a916-243c59dd04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fdea5e-9d8f-4bf6-ac8a-a94cb757faf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cae4e-0802-4da8-a6f6-971eb7276c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1e4ec-7ed9-48b9-a711-3c0e46f10b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87510a-203f-4323-8296-3d4974bf652a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"facebook/multilingual_librispeech\", \"dutch\", split=\"validation\", streaming=True)\n",
    "sample = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad820065-35ab-4a6b-a7eb-d1963c741e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sample[\"text\"])\n",
    "IPython.display.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558382a-6d3e-4068-8f9e-59e3239ec584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\":\"transcribe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb430b-cd43-4eee-8069-c92fe5568340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\":\"translate\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea787e-aeeb-4251-be6c-daa709369352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"facebook/multilingual_librispeech\", \"spanish\", split=\"validation\", streaming=True)\n",
    "sample = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca9664-c583-4f47-9cef-f00c2683290e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sample[\"text\"])\n",
    "IPython.display.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efa031-2bec-4464-88a3-77069ee5ac39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\":\"transcribe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100704af-398b-4572-a105-7b87ffe22407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\":\"translate\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8407c18-b830-4d16-8e98-bf7a679c5579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target_length_in_m = 5\n",
    "# convert from minutes to seconds(* 60) to num samples(* sampling_rate)\n",
    "sampling_rate = pipe.feature_extractor.sampling_rate\n",
    "target_length_in_samples = target_length_in_m * 60 * sampling_rate\n",
    "long_audio = []\n",
    "for sample in dataset:\n",
    "    long_audio.extend(sample[\"audio\"][\"array\"])\n",
    "    if len(long_audio) > target_length_in_samples:\n",
    "        break\n",
    "        \n",
    "long_audio = np.asarray(long_audio)\n",
    "\n",
    "# how did we do\n",
    "\n",
    "seconds = len(long_audio) / 16_000\n",
    "minutes, seconds = divmod(seconds, 60)\n",
    "\n",
    "print(f\"Length of audio sample is {minutes} minutes {seconds:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a5e23-b06e-475a-81fb-4f8b6506b711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe (\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30, # 30 second chunks\n",
    "    batch_size=8 # batch of 8 chunks at a time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d583442-8d52-4a24-954d-7dd9d96c8ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe (\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30, # 30 second chunks\n",
    "    batch_size=8, # batch of 8 chunks at a time\n",
    "    return_timestamps=True #return timestamps for annotating video\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7694e-bc70-48d7-97e2-0ebbe90d9570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference = \"the cat sat on the mat\"\n",
    "prediction = \"the cat sit on the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88c921-7c03-4b06-87f8-95c164f89f70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "wer_metric = load(\"wer\")\n",
    "wer = wer_metric.compute(references=[reference], predictions=[prediction])\n",
    "wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1195f0-c179-48b1-8986-acc09ab295a6",
   "metadata": {},
   "source": [
    "W Acc = 1 - WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a1cd5-9f39-4562-a162-62c72cc006c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cer_metric = load(\"cer\")\n",
    "cer = cer_metric.compute(references=[reference], predictions=[prediction])\n",
    "cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3ba4a-9d4f-4914-a6a5-fc43d3dedf67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "normalizer = BasicTextNormalizer()\n",
    "prediction = \" He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.\"\n",
    "normalized_prediction = normalizer(prediction)\n",
    "normalized_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63b367-0688-48b8-85eb-882fc247f5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference = \"HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\"\n",
    "normalized_reference = normalizer(reference)\n",
    "\n",
    "wer = wer_metric.compute(\n",
    "    references=[normalized_reference], predictions=[normalized_prediction]\n",
    ")\n",
    "wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837d27a-d360-4fc3-bbe4-75d48bf378f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "    torch_dtype=torch.float32\n",
    "    \n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa1fb6-98cf-47f3-b63f-5b907801d6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "common_voice_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faae6e6-5120-487e-a238-65774387b73c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "all_predictions =[]\n",
    "\n",
    "# rucommon_voice_testreamed inference\n",
    "for prediction in tqdm(\n",
    "    pipe(\n",
    "        KeyDataset(common_voice_test, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\"},\n",
    "        batch_size=32,\n",
    "    ),\n",
    "    total=len(common_voice_test),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a4bbc-e6ec-44d3-b0ac-da5a117e12b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "wer_metric = load(\"wer\")\n",
    "wer_ortho = 100* wer_metric.compute(\n",
    "    references=common_voice_test[\"sentence\"], predictions=all_predictions\n",
    ")\n",
    "wer_ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bb4d7-d2cb-4107-bb75-b452951df532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf195a-abbf-48ae-906f-ff6ad7f62547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute normalised WER\n",
    "all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
    "all_references_norm = [normalizer(label) for label in common_voice_test[\"sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5e319-eb1f-447c-901f-eade6dbacac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtering step to only evaluate the samples that correspond to non-zero-references\n",
    "all_predictions_norm = [\n",
    "    all_predictions_norm[i] for i in range(len(all_predictions_norm)) if len(all_references_norm[i]) > 0\n",
    "]\n",
    "all_references_norm = [\n",
    "    all_references_norm[i] for i in range(len(all_references_norm)) if len(all_references_norm[i]) > 0\n",
    "]\n",
    "wer = 100 * wer_metric.compute(\n",
    "    references=all_references_norm, predictions=all_predictions_norm\n",
    ")\n",
    "\n",
    "wer\n",
    "                                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1cb43-4fdd-41da-9729-ffd24063f1c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93abfd-5126-4302-b933-45be8cf01b95",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b13c5d-766a-4dd9-aeb8-bc5b661f7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d416827-5724-474c-b4cd-136c50cc2ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "minds_dataset = minds.train_test_split(len(minds) - 450)\n",
    "minds_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63f68cd-e5a8-48ea-ba7d-75b9cd4d26cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minds_dataset = minds_dataset.remove_columns(['lang_id', 'path', 'english_transcription', 'intent_class'])\n",
    "minds_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4b199-c68e-45c0-ba9e-21aab966480c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Extractor, Tokenizer and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1fcb7-ad03-420d-af68-ff68b43a5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
    "TO_LANGUAGE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac1726-ab77-4114-b27b-f31931ff6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English is closest\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"english\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2045f4-ba78-4358-a55c-a08476a2c704",
   "metadata": {},
   "source": [
    "### Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed72ab-5634-475a-ba41-ea8095f51be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "minds_dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549642d9-f1ee-4cc2-a002-0f85230a4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "sampling_rate=processor.feature_extractor.sampling_rate\n",
    "common_voice = minds_dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8fa95-1cb1-4a73-b911-728ce3b0d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and resample\n",
    "# compute log mel spectrogram\n",
    "# encode transcriptions to label ids with tokenizer\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "    example = processor(audio=audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], text=example[\"transcription\"])\n",
    "    # comppute input length of audio sample inseconds\n",
    "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "    \n",
    "    return example\n",
    "minds_dataset = minds_dataset.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c98ac28-b19f-4314-aff8-c0e68e991eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 30.0\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length\n",
    "\n",
    "# filter\n",
    "\n",
    "minds_dataset[\"train\"] = minds_dataset[\"train\"].filter(is_audio_in_length_range, input_columns=[\"input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf391c-af42-4242-8cc1-31528a57fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "minds_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da1b40-f43b-4b9f-831a-e4da43927d8a",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c754b41-8482-441e-b24d-4dea4fc014ed",
   "metadata": {},
   "source": [
    "- Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n",
    "- Evaluation metrics: during evaluation, we want to evaluate the model using the word error rate (WER) metric. We need to define a compute_metrics function that handles this computation.\n",
    "- Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
    "- Define the training arguments: these will be used by the 🤗 Trainer in constructing the training schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9191c-f41f-440a-a0b1-d86affb3921a",
   "metadata": {},
   "source": [
    "#### Define a Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa7bfd-9bcc-4c96-853a-2e8a0faff8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    \n",
    "    def __call__(self, features:List[Dict[str,Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\":feature[\"input_features\"][0]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        # get tokenized label sequences\n",
    "        label_features =[{\"input_ids\":feature[\"labels\"]} for feature in features]\n",
    "        # pad labels\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        # replace padding with -100\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        # remove start of transcript token appended by tokenizer as it's appended later anyway\n",
    "        if(labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297ed6d-8293-434a-9234-068db97cd085",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac86e4e-3d71-4655-958c-b13c793f5096",
   "metadata": {},
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7bf000-6b77-49a1-8fdb-b7dfee3dc1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f31de0-3251-4c7d-a655-dee55f2c5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb22a3f-9cf5-49ad-bffd-adcd7084d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    \n",
    "    # replace -100 with pad token\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    # do not group tokens when computing metrics\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # compute ortographic wer\n",
    "    wer_ortho = 100* metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    # compute normalized wer\n",
    "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
    "    label_str_norm = [normalizer(label) for label in label_str]\n",
    "    \n",
    "    # only evaluate non-zero references\n",
    "    pred_str_norm = [pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0]\n",
    "    label_str_norm = [label_str_norm[i] for i in range(len(label_str_norm)) if len(label_str_norm[i]) > 0]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
    "\n",
    "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e977aa9-9735-4e61-b261-e9fde0a57cdd",
   "metadata": {},
   "source": [
    "#### Load a pre-trained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03d706-d8ea-42b4-baa6-dc1eccd75f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7161e6-1f22-4856-a2f4-feb7feb7f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# disable cache during training as it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(model.generate, language=\"en-US\", task=\"transcribe\", use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044604bc-2c59-41bc-9c09-ab0f9e56160e",
   "metadata": {},
   "source": [
    "### Define a training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163b8b0-d8c3-4e71-bbad-27eaf60ac632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "398fbc52-cec1-4e69-97e6-c7087716a6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 1 training_args = Seq2SeqTrainingArguments(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 │   </span>output_dir=<span style=\"color: #808000; text-decoration-color: #808000\">\"./whisper-tiny-en-US\"</span>, <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># name on HF Hub</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 │   </span>per_device_train_batch_size=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span>,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 │   </span>gradient_accumulation_steps=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># increase by 2 for every 2x decrease in batch size</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.9/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">training_args.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1360</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__post_init__</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1357 │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> (get_xla_device_type(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device) != <span style=\"color: #808000; text-decoration-color: #808000\">\"GPU\"</span>)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1358 │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fp16 <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fp16_full_eval)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1359 │   │   </span>):                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1360 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1361 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1362 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" (`--fp16_full_eval`) can only be used on CUDA devices.\"</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1363 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>FP16 Mixed precision training with AMP or APEX <span style=\"font-weight: bold\">(</span>`--fp16`<span style=\"font-weight: bold\">)</span> and FP16 half precision evaluation \n",
       "<span style=\"font-weight: bold\">(</span>`--fp16_full_eval`<span style=\"font-weight: bold\">)</span> can only be used on CUDA devices.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 1 training_args = Seq2SeqTrainingArguments(                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0m\u001b[2m│   \u001b[0moutput_dir=\u001b[33m\"\u001b[0m\u001b[33m./whisper-tiny-en-US\u001b[0m\u001b[33m\"\u001b[0m, \u001b[2m# name on HF Hub\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[2m│   \u001b[0mper_device_train_batch_size=\u001b[94m16\u001b[0m,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   \u001b[0mgradient_accumulation_steps=\u001b[94m1\u001b[0m, \u001b[2m# increase by 2 for every 2x decrease in batch size\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m__init__\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/\u001b[0m\u001b[1;33mtraining_args.py\u001b[0m:\u001b[94m1360\u001b[0m in \u001b[92m__post_init__\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1357 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[95mand\u001b[0m (get_xla_device_type(\u001b[96mself\u001b[0m.device) != \u001b[33m\"\u001b[0m\u001b[33mGPU\u001b[0m\u001b[33m\"\u001b[0m)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1358 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[95mand\u001b[0m (\u001b[96mself\u001b[0m.fp16 \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.fp16_full_eval)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1359 \u001b[0m\u001b[2m│   │   \u001b[0m):                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1360 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1361 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1362 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[0m\u001b[33m\"\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1363 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mFP16 Mixed precision training with AMP or APEX \u001b[1m(\u001b[0m`--fp16`\u001b[1m)\u001b[0m and FP16 half precision evaluation \n",
       "\u001b[1m(\u001b[0m`--fp16_full_eval`\u001b[1m)\u001b[0m can only be used on CUDA devices.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-tiny-en-US\", # name on HF Hub\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1, # increase by 2 for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=50,\n",
    "    max_steps=4000, # decrease to 500 if you don't have your own GPU or a Colab paid plan or equivalent\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"wandb\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aac65a-65ec-4860-bf56-0d369242be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06de2ea-e250-4d32-9da4-1d8315e11ba1",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074dd210-15d5-4972-9389-16c28dbfce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d904eb2e-4f74-4c9b-92e1-35c7fb8384f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba40150-edd2-4e19-820c-e1bd0d532863",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_13_0\",\n",
    "    \"dataset\": \"Common Voice 13\",  # a 'pretty' name for the training dataset\n",
    "    \"language\": \"dv\",\n",
    "    \"model_name\": \"Whisper Small Dv - Peter Gelderbloem\",  # a 'pretty' name for your model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b6f72-577f-4ce4-9913-ad014853ff55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
